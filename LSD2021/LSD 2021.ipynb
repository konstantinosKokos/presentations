{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "center-classics",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<p>\n",
    "    <center>\n",
    "        <h1>From text to meaning</h1>\n",
    "        <h3>A type-driven Dutch treebank and its applications</h2>\n",
    "    </center>\n",
    "</p>\n",
    "\n",
    "\n",
    "<p>\n",
    "    <center>\n",
    "    <img src=\"lsd.jpeg\" width=\"400\"/>\n",
    "        <h4>LSD 2021</h4>\n",
    "        <h4>Kokos</h4>\n",
    "    </center>\n",
    "</p>\n",
    "\n",
    "\n",
    "> <div style=\"text-align: left\">This presentation and instructions to run it @ <a href=\"https://github.com/konstantinosKokos/Presentations/LSD2021\">github.com/konstantinosKokos/Presentations/LSD2021</a></div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "assigned-fortune",
   "metadata": {},
   "source": [
    "\n",
    "# Tiny bit of theory\n",
    "\n",
    "--------\n",
    "\n",
    "\n",
    "### The simply typed linear lambda calculus \\\\(\\lambda^\\multimap \\\\)\n",
    "Terms of the form\n",
    "\\\\[ \n",
    "t = var^{\\alpha} \\  | \\ {(t_1^{\\alpha \\multimap \\beta} \\ t_2^{\\alpha})}^\\beta \\ | \\ {(\\lambda x^{\\alpha}.t^{\\beta})}^{\\alpha \\multimap \\beta} \\\\]\n",
    "\n",
    "#### Can capture FA-structures in tectosyntax..\n",
    "\n",
    "<img src=\"syn.jpg\" width=\"650\"/>\n",
    "\n",
    "\n",
    "\\\\[\n",
    "\t\t\\left(\n",
    "\t\tbelieved^{s\\multimap np \\multimap s}\n",
    "\t\t\\left(\n",
    "\t\t\t\\left(\n",
    "\t\t\twas^{np \\multimap np \\multimap s}\n",
    "\t\t\t\\\n",
    "\t\t\t\t\\left(\n",
    "\t\t\t\t{a\\ witch}\n",
    "\t\t\t\t\\right)^{np}\n",
    "\t\t\t\\right)\\\n",
    "\t\t\t\\left(\n",
    "\t\t\t{his\\ neighbor}\n",
    "\t\t\t\\right)^{np}\n",
    "\t\t\\right)\n",
    "\t\t\\right)\n",
    "\t\t\\\n",
    "\t\t{Albert}^{np}\n",
    "\\\\]\n",
    "\n",
    "#### ..inlcuding higher-order phenomena (!)..\n",
    "\n",
    "<img src=\"shapes.jpg\" width=\"800\"/>\n",
    "\n",
    "\\\\[\n",
    "    \\left(\n",
    "        persisted^{np\\multimap s}\n",
    "        \\left( \n",
    "            behind^{np\\multimap (np\\multimap s) \\multimap np \\multimap s}\n",
    "            \\left( \n",
    "                closed^{np\\multimap np} \n",
    "                eyes^{np}\n",
    "             \\right)\n",
    "        \\right)\n",
    "    \\right)\\\n",
    "    \\left(\n",
    "         \\left(\n",
    "             that^{(np\\multimap s)\\multimap np\\multimap np} \n",
    "             \\left(\n",
    "                 \\lambda x.\n",
    "                     \\left(\n",
    "                         saw^{np\\multimap np\\multimap s}\n",
    "                         x^{np}\n",
    "                     \\right)\\\n",
    "                     he^{np}\n",
    "              \\right)\n",
    "         \\right)\n",
    "         \\left(\n",
    "            the^{np\\multimap np} shapes^{np}\n",
    "         \\right)\n",
    "     \\right)\n",
    "\\\\]\n",
    "\n",
    "#### ..but makes no distinction between signature-sharing words\n",
    "\n",
    "is, saw :: \\\\(np\\multimap np \\multimap s \\\\)\n",
    "\n",
    "\n",
    "\n",
    "### Adding modalities\n",
    "\n",
    "\\\\[  \n",
    "t = var^{\\alpha} \\  | \\ \n",
    "t_1^{\\diamondsuit^\\delta \\alpha \\multimap \\beta} \\ {\\vartriangle^\\delta} t_2^{\\alpha} \\ | \\\n",
    "\\blacktriangledown^\\delta \\!\\left(t_1^{\\Box^\\delta (\\alpha \\multimap \\beta)}\\right) \\ t_2^{\\alpha} \\ | \\ ... \n",
    "\\\\]\n",
    "\n",
    "is :: \\\\( \\diamondsuit^{predc}np \\multimap \\diamondsuit^{su}np \\multimap s \\\\)\n",
    "\n",
    "\\\\[\n",
    "     \\left(\n",
    "         is \\\n",
    "         \\vartriangle^{predc}\\!\\! \\left(a\\ witch\\right)\n",
    "     \\right)\n",
    "     \\vartriangle^{su}\\!\\! \\left( she \\right)\n",
    "\\\\]\n",
    "\n",
    "saw :: \\\\( \\diamondsuit^{obj}np \\multimap \\diamondsuit^{su}np \\multimap s \\\\)\n",
    "      \n",
    "\\\\[\n",
    "     \\left(\n",
    "         saw \\\n",
    "         \\vartriangle^{obj}\\!\\! (shapes)\n",
    "     \\right)\n",
    "     \\vartriangle^{su}\\!\\! \\left(he \\right)\n",
    "\\\\]\n",
    "\n",
    "\n",
    "closed :: \\\\(\\Box^{mod} (np \\multimap np) \\\\)\n",
    "\n",
    "\\\\[\n",
    "    \\blacktriangledown^{mod}(closed) \\ eyes\n",
    "\\\\]\n",
    "\n",
    "    ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5062f5a",
   "metadata": {},
   "source": [
    "# æthel\n",
    "\n",
    "> <div style=\"text-align: right\"><em>automatically extracted theorems from lassy</em></div>\n",
    "\n",
    "---\n",
    "\n",
    "A conversion of Lassy annotations to tecto-grammatic proofs & terms of the above logic.\n",
    "\n",
    "\n",
    "- access & installation instructions: \n",
    "[github.com/konstantinosKokos/lassy-tlg-extraction](https://github.com/konstantinosKokos/Lassy-TLG-Extraction)\n",
    "- paper: \n",
    "[aclweb.org/anthology/2020.lrec-1.647/](https://www.aclweb.org/anthology/2020.lrec-1.647/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eed95949",
   "metadata": {},
   "source": [
    "### First step: loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b78b10d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('/home/kokos/Projects/lassy-tlg-extraction/data/train_dev_test_0.4.dev0.p', 'rb') as f:\n",
    "    proofs = sum(pickle.load(f), [])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "775c8ebd",
   "metadata": {},
   "source": [
    "#### What does a `proof` look like..?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "acdb0644",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ProofNet(proof_frame=Alle:□ᵈᵉᵗ(ɴ → ɴᴘ), films:ɴ, zijn:◊ᵛᶜᴘᴘᴀʀᴛ → ◊ˢᵘɴᴘ → sᴍᴀɪɴ, heel:□ᵐᵒᵈ(ᴀᴘ → ᴀᴘ), barok:ᴀᴘ, versierd:◊ᵖʳᵉᵈᶜᴀᴘ → ᴘᴘᴀʀᴛ ⊢ sᴍᴀɪɴ, axiom_links={(5, 11), (2, 0), (1, 4), (7, 9), (8, 6), (10, 3)}, name='Treebank/WR-P-E-I-0000000332/WR-P-E-I-0000000332.p.4.s.98_0.xml')\n"
     ]
    }
   ],
   "source": [
    "some_proof = proofs[1312]\n",
    "print(some_proof)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eabdc56b",
   "metadata": {},
   "source": [
    "##### ..a proof frame\n",
    "A sequence of words & their corresponding *types*, together with a *conclusion* specifying the type of the entire phrase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6de58a6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alle:□ᵈᵉᵗ(ɴ → ɴᴘ), films:ɴ, zijn:◊ᵛᶜᴘᴘᴀʀᴛ → ◊ˢᵘɴᴘ → sᴍᴀɪɴ, heel:□ᵐᵒᵈ(ᴀᴘ → ᴀᴘ), barok:ᴀᴘ, versierd:◊ᵖʳᵉᵈᶜᴀᴘ → ᴘᴘᴀʀᴛ ⊢ sᴍᴀɪɴ\n"
     ]
    }
   ],
   "source": [
    "print(some_proof.proof_frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a334650f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Alle', 'films', 'zijn', 'heel', 'barok', 'versierd']\n"
     ]
    }
   ],
   "source": [
    "print(some_proof.proof_frame.get_words())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "572a7a12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[□ᵈᵉᵗ(ɴ(-,0) → ɴᴘ(+,1)), ɴ(+,2), ◊ᵛᶜᴘᴘᴀʀᴛ(-,3) → ◊ˢᵘɴᴘ(-,4) → sᴍᴀɪɴ(+,5), □ᵐᵒᵈ(ᴀᴘ(-,6) → ᴀᴘ(+,7)), ᴀᴘ(+,8), ◊ᵖʳᵉᵈᶜᴀᴘ(-,9) → ᴘᴘᴀʀᴛ(+,10)]\n"
     ]
    }
   ],
   "source": [
    "print(some_proof.proof_frame.get_types())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "18c4cf62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sᴍᴀɪɴ(-,11)\n"
     ]
    }
   ],
   "source": [
    "print(some_proof.proof_frame.conclusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3053adc3",
   "metadata": {},
   "source": [
    "##### .. some axiom links\n",
    "a bijection between *positive* and *negative* atoms (a compressed representation of a proof)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b4c6b597",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{(5, 11), (2, 0), (1, 4), (7, 9), (8, 6), (10, 3)}\n"
     ]
    }
   ],
   "source": [
    "print(some_proof.axiom_links)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe5c314",
   "metadata": {},
   "source": [
    "##### ..and a name\n",
    "(optionally) telling us its lassy origins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d07bea0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treebank/WR-P-E-I-0000000332/WR-P-E-I-0000000332.p.4.s.98_0.xml\n"
     ]
    }
   ],
   "source": [
    "print(some_proof.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21355cb3",
   "metadata": {},
   "source": [
    "#### Proofs ≡ terms \n",
    "An intuitionistic logic proof is one and the same to a λ-term (or *program*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "78bd37ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'((zijn ▵ᵛᶜ((versierd ▵ᵖʳᵉᵈᶜ((▾ᵐᵒᵈ(heel) barok))))) ▵ˢᵘ((▾ᵈᵉᵗ(Alle) films)))'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "some_proof.print_term(show_words=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88a73833",
   "metadata": {},
   "source": [
    "..modalities (& term decorations) can be dropped to fall back to simple \\\\(\\lambda^\\multimap \\\\) terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "406061c0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'((zijn (versierd (heel barok))) (Alle films))'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "some_proof.print_term(show_words=True, show_decorations=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79e1024e",
   "metadata": {},
   "source": [
    "### The Lexicon\n",
    "From the full collection of proof frames we can aggregate a mapping **Word** \\\\( \\to \\\\) **{Type}**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9e7e1da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from LassyExtraction.aethel import ProofNet\n",
    "from LassyExtraction.milltypes import WordType\n",
    "\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "def make_lexicon(proofs: list[ProofNet], simple: bool = False) -> dict[str, Counter[WordType]]:\n",
    "    c = defaultdict(lambda: Counter())\n",
    "    for pn in proofs:\n",
    "        for word, wordtype in zip(pn.proof_frame.get_words(), pn.proof_frame.get_types()):\n",
    "            wordtype = wordtype.depolarize()\n",
    "            c[word.lower()][wordtype.decolor() if simple else wordtype] += 1\n",
    "    return c\n",
    "\n",
    "simple_lexicon = make_lexicon(proofs, True)\n",
    "deco_lexicon = make_lexicon(proofs, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8215c53e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({ᴡᴡ: 20,\n",
       "         ɪɴғ: 11,\n",
       "         ᴠᴢ → ɪɴғ: 3,\n",
       "         ʙᴡ → ɪɴғ: 1,\n",
       "         ɴᴘ → ɪɴғ: 6,\n",
       "         ᴠɴᴡ → ɪɴғ: 1,\n",
       "         ᴀᴅᴊ → ssᴜʙ: 1,\n",
       "         ɴᴘ: 3,\n",
       "         ᴘᴘ → ɪɴғ: 2,\n",
       "         ᴠᴢ → ɴᴘ → ɪɴғ: 1,\n",
       "         ᴡʜsᴜʙ → ɪɴғ: 4,\n",
       "         ᴠᴢ → ʙᴡ → ɴᴘ → ɪɴғ: 1})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_lexicon['lezen']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce6bb335",
   "metadata": {},
   "source": [
    "### Querying the dataset\n",
    "We can filter sentences satisfying arbitrary predicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cfe42a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable\n",
    "\n",
    "def get_proofs(proofs: list[ProofNet], predicate: Callable[[ProofNet], bool]) -> list[ProofNet]:\n",
    "    return list(filter(predicate, proofs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25d8bcd8",
   "metadata": {},
   "source": [
    "..like containing a specific word, word-type combination, dependency label, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a5709254",
   "metadata": {},
   "outputs": [],
   "source": [
    "def containing_word(word: str) -> Callable[[ProofNet], bool]:\n",
    "    def f(pn: ProofNet) -> bool:\n",
    "        return word in (w.lower() for w in pn.proof_frame.get_words())\n",
    "    return f\n",
    "\n",
    "def containing_dep(dep: str) -> Callable[[ProofNet], bool]:\n",
    "    def f(pn: ProofNet) -> bool:\n",
    "        return dep in set.union(*[t.colors() for t in pn.proof_frame.get_types()])\n",
    "    return f\n",
    "\n",
    "def containing_word_as(word: str, wordtype: WordType) -> Callable[[ProofNet], bool]:\n",
    "    def f(pn: ProofNet) -> bool:\n",
    "        return any(map(lambda w, t: w.lower() == word and t.depolarize() == wordtype,\n",
    "                      pn.proof_frame.get_words(),\n",
    "                       pn.proof_frame.get_types()))\n",
    "    return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "09f41e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "from LassyExtraction.milltypes import AtomicType\n",
    "eenden = get_proofs(proofs, containing_word_as('eenden', AtomicType('N')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d78adbe0",
   "metadata": {},
   "source": [
    "### Checking against Lassy\n",
    "Proofs can be directly compared to the original Lassy annotations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a2403cc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignoring 156 samples..\n",
      "Dataset constructed with 65045 samples.\n"
     ]
    }
   ],
   "source": [
    "from LassyExtraction.lassy import Lassy, et\n",
    "from LassyExtraction.viz import ToGraphViz\n",
    "\n",
    "\n",
    "lassy = Lassy(ignore='/home/kokos/Projects/lassy-tlg-extraction/LassyExtraction/utils/ignored.txt')\n",
    "viz = ToGraphViz()\n",
    "\n",
    "\n",
    "def find_in_lassy(name: str) -> et:\n",
    "    source, _ = name.split('_')\n",
    "    return lassy[f'/home/kokos/Projects/Lassy 4.0/LassySmall/{source}.xml'][2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "eb23e96c",
   "metadata": {},
   "outputs": [],
   "source": [
    "lassy_tree = find_in_lassy(eenden[0].name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cc8d16fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "lassy_tree = find_in_lassy('Treebank/dpc-ind-001645-nl-sen/dpc-ind-001645-nl-sen.p.12.s.1_1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "966bb8d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "viz(lassy_tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18511502",
   "metadata": {},
   "source": [
    "# Neural Proof Nets\n",
    "\n",
    "---\n",
    "\n",
    "A neural parser based on a *seq2seq* module to translate phrases to proof frames and a *permutation* module to align positive & negative atoms\n",
    "\n",
    "- access & installation instructions:\n",
    "[github.com/konstantinosKokos/neural-proof-nets](https://github.com/konstantinosKokos/neural-proof-nets)\n",
    "- paper:\n",
    "[aclweb.org/anthology/2020.conll-1.3/](https://www.aclweb.org/anthology/2020.conll-1.3/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "568091d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing model...\n",
      "Initialized.\n",
      "Loading pre-trained parameters...\n",
      "Loading model parameters...\n",
      "Loaded.\n"
     ]
    }
   ],
   "source": [
    "from Parser.neural.inference import get_model\n",
    "\n",
    "model = get_model('cuda')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "99a9dd71",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "model.infer(['De strategie die ze volgen is eeuwenoud'], 1)[0][0].to_proofnet().print_term(show_words=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2797a2ee",
   "metadata": {},
   "source": [
    "<img src=\"strategie.jpg\" width=\"700\"/>\n",
    "\n",
    "<img src=\"matrices.jpg\" width=\"700\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95428804",
   "metadata": {},
   "source": [
    "# -- fun but experimental part -- \n",
    "Computing semantics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "e474dca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from LassyExtraction.terms import *\n",
    "from LassyExtraction.milltypes import *\n",
    "from functools import partial\n",
    "from typing import Callable, TypeVar\n",
    "\n",
    "Meaning = TypeVar('Meaning')\n",
    "\n",
    "# some standard types\n",
    "_np, _s = AtomicType('NP'), AtomicType('S')\n",
    "_itv = FunctorType(_np, _s)\n",
    "_tv = FunctorType(_np, _itv)\n",
    "_adj = FunctorType(_np, _np)\n",
    "# -- corresponding terms\n",
    "np = lambda x: Lex(_type=_np, idx=x)\n",
    "itv = lambda x: Lex(_type=_itv, idx=x)\n",
    "tv = lambda x: Lex(_type=_tv, idx=x)\n",
    "adj = lambda x: Lex(_type=_adj, idx=x)\n",
    "# common sentence structures\n",
    "s1 = Application(itv(1), np(0))\n",
    "s2 = Application(Application(tv(1), np(2)), np(0))\n",
    "\n",
    "\n",
    "def meaning(term: Term, word_meanings: dict[int, Meaning]) -> Meaning:\n",
    "    if isinstance(term, Lex):\n",
    "        return word_meanings[term.idx]\n",
    "    if isinstance(term, Application):\n",
    "        return meaning(term.functor, word_meanings)(meaning(term.argument, word_meanings))\n",
    "    raise TypeError('only 0-order implicative fragment please!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "293738de",
   "metadata": {},
   "source": [
    "#### Take 1: model-theoretic semantics\n",
    "John likes Mary but Mary is a duck!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "cdae4d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "E = str\n",
    "T = bool\n",
    "ET = Callable[[E], T]\n",
    "EET = Callable[[E], ET]\n",
    "\n",
    "John:  E = 'John'\n",
    "Mary:  E = 'Mary'\n",
    "    \n",
    "talks: ET = lambda x: x == John\n",
    "swims: ET = lambda x: True\n",
    "flies: ET = lambda x: x == Mary\n",
    "human: ET = lambda x: talks(x)\n",
    "duck:  ET = lambda x: swims(x) and flies(x)\n",
    "    \n",
    "is_a:  EET = lambda y: lambda x: y(x)\n",
    "likes: EET = lambda y: lambda x: True if x == John and y == Mary else False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f54e0d23",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
