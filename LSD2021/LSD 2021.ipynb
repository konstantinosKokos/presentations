{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "center-classics",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<p>\n",
    "    <center>\n",
    "        <h1>From text to meaning</h1>\n",
    "        <h3>A type-driven Dutch treebank and its applications</h2>\n",
    "    </center>\n",
    "</p>\n",
    "\n",
    "\n",
    "<p>\n",
    "    <center>\n",
    "    <img src=\"lsd.jpeg\" width=\"400\"/>\n",
    "        <h4>LSD 2021</h4>\n",
    "        <h4>Kokos</h4>\n",
    "    </center>\n",
    "</p>\n",
    "\n",
    "\n",
    "> <div style=\"text-align: left\">This presentation and instructions to run it @ <a href=\"https://github.com/konstantinosKokos/Presentations/LSD2021\">github.com/konstantinosKokos/Presentations/LSD2021</a></div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "assigned-fortune",
   "metadata": {},
   "source": [
    "\n",
    "# Tiny bit of theory\n",
    "\n",
    "--------\n",
    "\n",
    "\n",
    "### The simply typed linear lambda calculus \\\\(\\lambda^\\multimap \\\\)\n",
    "Terms of the form\n",
    "\\\\[ \n",
    "t = var^{\\alpha} \\  | \\ {(t_1^{\\alpha \\multimap \\beta} \\ t_2^{\\alpha})}^\\beta \\ | \\ {(\\lambda x^{\\alpha}.t^{\\beta})}^{\\alpha \\multimap \\beta} \\\\]\n",
    "\n",
    "#### Can capture FA-structures in tectosyntax..\n",
    "\n",
    "<img src=\"syn.jpg\" width=\"650\"/>\n",
    "\n",
    "\n",
    "\\\\[\n",
    "\t\t\\left(\n",
    "\t\tbelieved^{s\\multimap np \\multimap s}\n",
    "\t\t\\left(\n",
    "\t\t\t\\left(\n",
    "\t\t\twas^{np \\multimap s}\n",
    "\t\t\t\\\n",
    "\t\t\t\t\\left(\n",
    "\t\t\t\t{a\\ witch}\n",
    "\t\t\t\t\\right)^{np}\n",
    "\t\t\t\\right)\\\n",
    "\t\t\t\\left(\n",
    "\t\t\t{his\\ neighbor}\n",
    "\t\t\t\\right)^{np}\n",
    "\t\t\\right)\n",
    "\t\t\\right)\n",
    "\t\t\\\n",
    "\t\t{Albert}^{np}\n",
    "\\\\]\n",
    "\n",
    "#### ..inlcuding higher-order phenomena (!)..\n",
    "\n",
    "<img src=\"shapes.jpg\" width=\"800\"/>\n",
    "\n",
    "\\\\[\n",
    "    \\left(\n",
    "        persisted^{np\\multimap s}\n",
    "        \\left( \n",
    "            behind^{np\\multimap (np\\multimap s) \\multimap np \\multimap s}\n",
    "            \\left( \n",
    "                closed^{np\\multimap np} \n",
    "                eyes^{np}\n",
    "             \\right)\n",
    "        \\right)\n",
    "    \\right)\\\n",
    "    \\left(\n",
    "         \\left(\n",
    "             that^{(np\\multimap s)\\multimap np\\multimap np} \n",
    "             \\left(\n",
    "                 \\lambda x.\n",
    "                     \\left(\n",
    "                         saw^{np\\multimap np\\multimap s}\n",
    "                         x^{np}\n",
    "                     \\right)\\\n",
    "                     he^{np}\n",
    "              \\right)\n",
    "         \\right)\n",
    "         \\left(\n",
    "            the^{np\\multimap np} shapes^{np}\n",
    "         \\right)\n",
    "     \\right)\n",
    "\\\\]\n",
    "\n",
    "#### ..but makes no distinction between signature-sharing words\n",
    "\n",
    "is, saw :: \\\\(np\\multimap np \\multimap s \\\\)\n",
    "\n",
    "\n",
    "\n",
    "### Adding modalities\n",
    "\n",
    "\\\\[  \n",
    "t = var^{\\alpha} \\  | \\ \n",
    "t_1^{\\diamondsuit^\\delta \\alpha \\multimap \\beta} \\ {\\vartriangle^\\delta} t_2^{\\alpha} \\ | \\\n",
    "\\blacktriangledown^\\delta \\!\\left(t_1^{\\Box^\\delta (\\alpha \\multimap \\beta)}\\right) \\ t_2^{\\alpha} \\ | \\ ... \n",
    "\\\\]\n",
    "\n",
    "is :: \\\\( \\diamondsuit^{predc}np \\multimap \\diamondsuit^{su}np \\multimap s \\\\)\n",
    "\n",
    "\\\\[\n",
    "     \\left(\n",
    "         is \\\n",
    "         \\vartriangle^{predc}\\!\\! \\left(a\\ witch\\right)\n",
    "     \\right)\n",
    "     \\vartriangle^{su}\\!\\! \\left( she \\right)\n",
    "\\\\]\n",
    "\n",
    "saw :: \\\\( \\diamondsuit^{obj}np \\multimap \\diamondsuit^{su}np \\multimap s \\\\)\n",
    "      \n",
    "\\\\[\n",
    "     \\left(\n",
    "         saw \\\n",
    "         \\vartriangle^{obj}\\!\\! (shapes)\n",
    "     \\right)\n",
    "     \\vartriangle^{su}\\!\\! \\left(he \\right)\n",
    "\\\\]\n",
    "\n",
    "\n",
    "closed :: \\\\(\\Box^{mod} (np \\multimap np) \\\\)\n",
    "\n",
    "\\\\[\n",
    "    \\blacktriangledown^{mod}(closed) \\ eyes\n",
    "\\\\]\n",
    "\n",
    "    ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5062f5a",
   "metadata": {},
   "source": [
    "# æthel\n",
    "\n",
    "> <div style=\"text-align: right\"><em>automatically extracted theorems from lassy</em></div>\n",
    "\n",
    "---\n",
    "\n",
    "A conversion of Lassy annotations to tecto-grammatic proofs & terms of the above logic.\n",
    "\n",
    "\n",
    "- access & installation instructions: \n",
    "[github.com/konstantinosKokos/lassy-tlg-extraction](https://github.com/konstantinosKokos/Lassy-TLG-Extraction)\n",
    "- paper: \n",
    "[aclweb.org/anthology/2020.lrec-1.647/](https://www.aclweb.org/anthology/2020.lrec-1.647/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eed95949",
   "metadata": {},
   "source": [
    "### First step: loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3b78b10d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('/home/kokos/Projects/lassy-tlg-extraction/data/train_dev_test_0.4.dev0.p', 'rb') as f:\n",
    "    proofs = sum(pickle.load(f), [])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "775c8ebd",
   "metadata": {},
   "source": [
    "#### What does a `proof` look like..?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "acdb0644",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ProofNet(proof_frame=Alle:□ᵈᵉᵗ(ɴ → ɴᴘ), films:ɴ, zijn:◊ᵛᶜᴘᴘᴀʀᴛ → ◊ˢᵘɴᴘ → sᴍᴀɪɴ, heel:□ᵐᵒᵈ(ᴀᴘ → ᴀᴘ), barok:ᴀᴘ, versierd:◊ᵖʳᵉᵈᶜᴀᴘ → ᴘᴘᴀʀᴛ ⊢ sᴍᴀɪɴ, axiom_links={(5, 11), (2, 0), (1, 4), (7, 9), (8, 6), (10, 3)}, name='Treebank/WR-P-E-I-0000000332/WR-P-E-I-0000000332.p.4.s.98_0.xml')\n"
     ]
    }
   ],
   "source": [
    "some_proof = proofs[1312]\n",
    "print(some_proof)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eabdc56b",
   "metadata": {},
   "source": [
    "##### ..a proof frame\n",
    "A sequence of words & their corresponding *types*, together with a *conclusion* specifying the type of the entire phrase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6de58a6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alle:□ᵈᵉᵗ(ɴ → ɴᴘ), films:ɴ, zijn:◊ᵛᶜᴘᴘᴀʀᴛ → ◊ˢᵘɴᴘ → sᴍᴀɪɴ, heel:□ᵐᵒᵈ(ᴀᴘ → ᴀᴘ), barok:ᴀᴘ, versierd:◊ᵖʳᵉᵈᶜᴀᴘ → ᴘᴘᴀʀᴛ ⊢ sᴍᴀɪɴ\n"
     ]
    }
   ],
   "source": [
    "print(some_proof.proof_frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a334650f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Alle', 'films', 'zijn', 'heel', 'barok', 'versierd']\n"
     ]
    }
   ],
   "source": [
    "print(some_proof.proof_frame.get_words())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "572a7a12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[□ᵈᵉᵗ(ɴ(-,0) → ɴᴘ(+,1)), ɴ(+,2), ◊ᵛᶜᴘᴘᴀʀᴛ(-,3) → ◊ˢᵘɴᴘ(-,4) → sᴍᴀɪɴ(+,5), □ᵐᵒᵈ(ᴀᴘ(-,6) → ᴀᴘ(+,7)), ᴀᴘ(+,8), ◊ᵖʳᵉᵈᶜᴀᴘ(-,9) → ᴘᴘᴀʀᴛ(+,10)]\n"
     ]
    }
   ],
   "source": [
    "print(some_proof.proof_frame.get_types())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "18c4cf62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sᴍᴀɪɴ(-,11)\n"
     ]
    }
   ],
   "source": [
    "print(some_proof.proof_frame.conclusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3053adc3",
   "metadata": {},
   "source": [
    "##### .. some axiom links\n",
    "a bijection between *positive* and *negative* atoms (a compressed representation of a proof)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b4c6b597",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{(5, 11), (2, 0), (1, 4), (7, 9), (8, 6), (10, 3)}\n"
     ]
    }
   ],
   "source": [
    "print(some_proof.axiom_links)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe5c314",
   "metadata": {},
   "source": [
    "##### ..and a name\n",
    "(optionally) telling us its lassy origins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d07bea0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treebank/WR-P-E-I-0000000332/WR-P-E-I-0000000332.p.4.s.98_0.xml\n"
     ]
    }
   ],
   "source": [
    "print(some_proof.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21355cb3",
   "metadata": {},
   "source": [
    "#### Proofs ≡ terms \n",
    "An intuitionistic logic proof is one and the same to a λ-term (or *program*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "78bd37ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'((zijn ▵ᵛᶜ((versierd ▵ᵖʳᵉᵈᶜ((▾ᵐᵒᵈ(heel) barok))))) ▵ˢᵘ((▾ᵈᵉᵗ(Alle) films)))'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "some_proof.print_term(show_words=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88a73833",
   "metadata": {},
   "source": [
    "..modalities (& term decorations) can be dropped to fall back to simple \\\\(\\lambda^\\multimap \\\\) terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "406061c0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'((zijn (versierd (heel barok))) (Alle films))'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "some_proof.print_term(show_words=True, show_decorations=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79e1024e",
   "metadata": {},
   "source": [
    "### The Lexicon\n",
    "From the full collection of proof frames we can aggregate a mapping **Word** \\\\( \\to \\\\) **{Type}**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9e7e1da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from LassyExtraction.aethel import ProofNet\n",
    "from LassyExtraction.milltypes import WordType\n",
    "\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "def make_lexicon(proofs: list[ProofNet], simple: bool = False) -> dict[str, Counter[WordType]]:\n",
    "    c = defaultdict(lambda: Counter())\n",
    "    for pn in proofs:\n",
    "        for word, wordtype in zip(pn.proof_frame.get_words(), pn.proof_frame.get_types()):\n",
    "            wordtype = wordtype.depolarize()\n",
    "            c[word.lower()][wordtype.decolor() if simple else wordtype] += 1\n",
    "    return c\n",
    "\n",
    "simple_lexicon = make_lexicon(proofs, True)\n",
    "deco_lexicon = make_lexicon(proofs, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "8215c53e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({ᴡᴡ: 20,\n",
       "         ɪɴғ: 11,\n",
       "         ᴠᴢ → ɪɴғ: 3,\n",
       "         ʙᴡ → ɪɴғ: 1,\n",
       "         ɴᴘ → ɪɴғ: 6,\n",
       "         ᴠɴᴡ → ɪɴғ: 1,\n",
       "         ᴀᴅᴊ → ssᴜʙ: 1,\n",
       "         ɴᴘ: 3,\n",
       "         ᴘᴘ → ɪɴғ: 2,\n",
       "         ᴠᴢ → ɴᴘ → ɪɴғ: 1,\n",
       "         ᴡʜsᴜʙ → ɪɴғ: 4,\n",
       "         ᴠᴢ → ʙᴡ → ɴᴘ → ɪɴғ: 1})"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_lexicon['lezen']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce6bb335",
   "metadata": {},
   "source": [
    "### Querying the dataset\n",
    "We can filter sentences satisfying arbitrary predicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cfe42a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable\n",
    "\n",
    "def get_proofs(proofs: list[ProofNet], predicate: Callable[[ProofNet], bool]) -> list[ProofNet]:\n",
    "    return list(filter(predicate, proofs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25d8bcd8",
   "metadata": {},
   "source": [
    "..like containing a specific word, word-type combination, dependency label, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "a5709254",
   "metadata": {},
   "outputs": [],
   "source": [
    "def containing_word(word: str) -> Callable[[ProofNet], bool]:\n",
    "    def f(pn: ProofNet) -> bool:\n",
    "        return word in (w.lower() for w in pn.proof_frame.get_words())\n",
    "    return f\n",
    "\n",
    "def containing_dep(dep: str) -> Callable[[ProofNet], bool]:\n",
    "    def f(pn: ProofNet) -> bool:\n",
    "        return dep in set.union(*[t.colors() for t in pn.proof_frame.get_types()])\n",
    "    return f\n",
    "\n",
    "def containing_word_as(word: str, wordtype: WordType) -> Callable[[ProofNet], bool]:\n",
    "    def f(pn: ProofNet) -> bool:\n",
    "        return any(map(lambda w, t: w.lower() == word and t.depolarize() == wordtype,\n",
    "                      pn.proof_frame.get_words(),\n",
    "                       pn.proof_frame.get_types()))\n",
    "    return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "09f41e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "from LassyExtraction.milltypes import AtomicType\n",
    "eenden = get_proofs(proofs, containing_word_as('eenden', AtomicType('N')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d78adbe0",
   "metadata": {},
   "source": [
    "### Checking against Lassy\n",
    "Proofs can be directly compared to the original Lassy annotations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a2403cc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignoring 156 samples..\n",
      "Dataset constructed with 65045 samples.\n"
     ]
    }
   ],
   "source": [
    "from LassyExtraction.lassy import Lassy, et\n",
    "from LassyExtraction.viz import ToGraphViz\n",
    "\n",
    "\n",
    "lassy = Lassy(ignore='/home/kokos/Projects/lassy-tlg-extraction/LassyExtraction/utils/ignored.txt')\n",
    "viz = ToGraphViz()\n",
    "\n",
    "\n",
    "def find_in_lassy(name: str) -> et:\n",
    "    source, _ = name.split('_')\n",
    "    return lassy[f'/home/kokos/Projects/Lassy 4.0/LassySmall/{source}.xml'][2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "eb23e96c",
   "metadata": {},
   "outputs": [],
   "source": [
    "lassy_tree = find_in_lassy(eenden[0].name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "966bb8d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "viz(lassy_tree)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "24b1df3f",
   "metadata": {},
   "source": [
    "# Neural Proof Nets\n",
    "\n",
    "---\n",
    "\n",
    "A neural parser based on a *seq2seq* module to translate phrases to proof frames and a *permutation* module to align positive & negative atoms\n",
    "\n",
    "- access & installation instructions:\n",
    "[github.com/konstantinosKokos/neural-proof-nets](https://github.com/konstantinosKokos/neural-proof-nets)\n",
    "- paper:\n",
    "[aclweb.org/anthology/2020.conll-1.3/](https://www.aclweb.org/anthology/2020.conll-1.3/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abe93dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Parser.neural.inference import get_model\n",
    "\n",
    "model = get_model('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "54ac1212",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Analysis(words=['Die', 'strategie', 'die', 'ze', 'volgen', 'is', 'eeuwenoud'], types=[sᴍᴀɪɴ(-,15), □ᵈᵉᵗ(ɴ(-,0) → ɴᴘ(+,1)), ɴ(+,2), ◊ʳᵉˡᶜˡ(◊ᵒᵇʲ¹ᴠɴᴡ(+,3) → ssᴜʙ(-,4)) → □ᵐᵒᵈ(ɴᴘ(-,5) → ɴᴘ(+,6)), ᴠɴᴡ(+,7), ◊ᵒᵇʲ¹ᴠɴᴡ(-,8) → ◊ˢᵘᴠɴᴡ(-,9) → ssᴜʙ(+,10), ◊ᵖʳᵉᵈᶜᴀᴅᴊ(-,11) → ◊ˢᵘɴᴘ(-,12) → sᴍᴀɪɴ(+,13), ᴀᴅᴊ(+,14)], polish=['[SOS]', 'SMAIN', '[SEP]', '□det', 'N', 'NP', '[SEP]', 'N', '[SEP]', '◊relcl', '◊obj1', 'VNW', 'SSUB', '□mod', 'NP', 'NP', '[SEP]', 'VNW', '[SEP]', '◊obj1', 'VNW', '◊su', 'VNW', 'SSUB', '[SEP]', '◊predc', 'ADJ', '◊su', 'NP', 'SMAIN', '[SEP]', 'ADJ', '[SEP]'], atom_set=[ɴ, ssᴜʙ, ᴀᴅᴊ, ᴠɴᴡ, sᴍᴀɪɴ, ɴᴘ], pos_ids=[[7], [23], [31], [11, 17], [29], [5, 15]], neg_ids=[[4], [12], [26], [20, 22], [1], [14, 28]], idx_to_polish={1: 15, 4: 0, 5: 1, 7: 2, 11: 3, 12: 4, 14: 5, 15: 6, 17: 7, 20: 8, 22: 9, 23: 10, 26: 11, 28: 12, 29: 13, 31: 14}, link_weights=[[24.184823989868164], [-3.7895798683166504], [2.2147955894470215], [6.503294944763184, -6.495479106903076], [-9.490674018859863], [22.133684158325195, 2.1772096157073975]], axiom_links={2: 0, 10: 4, 14: 11, 3: 8, 7: 9, 13: 15, 1: 5, 6: 12}, traceback=None)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.infer(['Die strategie die ze volgen is eeuwenoud'], 1)[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96efef8d",
   "metadata": {},
   "source": [
    "<img src=\"strategie.jpg\" width=\"700\"/>\n",
    "\n",
    "<img src=\"matrices.jpg\" width=\"700\"/>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
